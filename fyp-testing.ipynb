{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load Libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path\nimport csv\n\n# For getting trained word embeddings\nfrom gensim.models import FastText\n\n# getting rid of Tensorflow warnings\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n# used for deep learning\nimport tensorflow as tf\nimport tensorflow.keras.layers as tfl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing\n### Load Data\nDefines function that load the data and returns it as tensors of latitude, longitude and all tweets for a twitter user. Also defines function that optimizes datasets.","metadata":{}},{"cell_type":"code","source":"# Code for optimizing dataset performance\nAUTOTUNE = tf.data.AUTOTUNE\ndef configure_dataset(dataset):\n  return dataset.cache().prefetch(buffer_size=AUTOTUNE)\n\n# Load file and return datasets for text and location and their combined training dataset\ndef loadData(data_path):\n    df = pd.read_csv(data_path, delimiter='\\t', encoding='utf-8', names=['user', 'lat', 'lon', 'text'],\n                     quoting=csv.QUOTE_NONE, on_bad_lines='skip')\n    # Get latitude, longitude and tweets separately\n    lat = df['lat']\n    lon = df ['lon']\n    text = df ['text']\n    \n    # Remove ||| separator and replace with space\n    text = text.str.lstrip(\" |\")\n    text = text.str.replace(\" \\|\\|\\| \", \" \", regex=True)\n    \n    # Convert all to tensor and return\n    lat = tf.convert_to_tensor(lat)\n    lon = tf.convert_to_tensor(lon)\n    text = tf.convert_to_tensor(text)\n    \n    # Convert to text dataset and location dataset respectively\n    text_ds = tf.data.Dataset.from_tensor_slices(text)\n    location_ds = tf.data.Dataset.from_tensor_slices((lat, lon))\n    trainingData = tf.data.Dataset.zip((text_ds, location_ds))\n\n    # Configure datasets for performance\n    text_ds = configure_dataset(text_ds)\n    location_ds = configure_dataset(location_ds)\n    trainingData = configure_dataset(trainingData)\n    \n    # Return datasets\n    return text_ds, location_ds, trainingData","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Tensors are then turned into datasets that can be split into batches.","metadata":{}},{"cell_type":"code","source":"# TODO: use training set\ndata_path = Path('/kaggle/input/geoworld/user_info.dev')\n\n# Get datasets from data file\ntext_ds, location_ds, trainingData = loadData(data_path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text Vectorization\nNow we have to convert each string of text in the data into an array of integers where every integer is an index that belongs to a word. This is done so that we can input data into our model.","metadata":{}},{"cell_type":"code","source":"# Turns each string of text into an array of integers\nMAX_SEQUENCE_LENGTH = 250\n\n# Create TextVectorization layer and build vocabulary from dataset\n# This layer takes an input array of strings of shape (batch_size) and outputs arrays of shape (batch_size, MAX_SEQUENCE_LENGTH)\nvectorize_layer = tfl.TextVectorization(output_sequence_length=MAX_SEQUENCE_LENGTH)\nvectorize_layer.adapt(text_ds.batch(128))\n\n# Save vocabulary and vocabulary size\nvocab = vectorize_layer.get_vocabulary()\nVOCAB_SIZE = len(vocab)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word Embeddings\nNext, we convert each integer in our data into a unique word embedding.  \nThe following function gets embeddings for all the words in our dataset's vocabulary and returns it as a matrix.","metadata":{}},{"cell_type":"code","source":"# Gets embeddings for words in the dataset. This assumes that TextVectorization has already built a vocabulary.\ndef get_embeddings():\n    # Create lexicons of the words 1->and\n    word_index = dict(zip(vocab, range(VOCAB_SIZE)))\n    \n    ft_model = FastText.load('/kaggle/input/testing-notebook/embeddings.model')\n    trained_embeddings = ft_model.wv\n    \n    embedding_dim = 100\n    hits = 0\n    misses = 0\n    \n    # Prepare embedding matrix by matching every word in embeddings with vectorizer\n    embedding_matrix = np.zeros((VOCAB_SIZE, embedding_dim))\n    for word, i in word_index.items():\n        embedding_vector = trained_embeddings.get_vector(word, norm=True)\n        \n        if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        \n            embedding_matrix[i] = embedding_vector\n            hits = hits + 1\n        else:\n            misses = misses + 1\n    print(\"Converted %d words (%d misses)\" % (hits, misses))\n    return embedding_matrix","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turns array of integers into an array of embeddings\n# Embedding Layer takes input array of shape (batch_size, MAX_SEQUENCE_LENGTH) and outputs array of shape (batch_size, MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)\nEMBEDDING_DIM = 100\nembedding_matrix = get_embeddings()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Model","metadata":{}},{"cell_type":"code","source":"# Embedding layer\nembedding_layer = tfl.Embedding(input_dim = VOCAB_SIZE, \n                                output_dim = EMBEDDING_DIM, \n                                input_length=MAX_SEQUENCE_LENGTH,\n                                embeddings_initializer = tf.keras.initializers.Constant(embedding_matrix),\n                                trainable=False)\n\n# Make model\ninputs = tfl.Input(shape=(None,), dtype=tf.string)\nx = vectorize_layer(inputs)\nx = embedding_layer(x)\nx = tfl.LSTM(1024, use_bias=True)(x)\n\nlat_pred = tfl.Dense(1, name='latitude')(x)\nlon_pred = tfl.Dense(1, name='longitude')(x)\n\nmodel = tf.keras.Model(inputs = inputs, outputs = [lat_pred, lon_pred])\nmodel.summary()\ntf.keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Compile Model","metadata":{}},{"cell_type":"code","source":"import tensorflow.keras.backend as K\n\ndef tilted_loss(q,y,f):\n    e = (y-f)\n    return K.mean(K.maximum(q*e, (q-1)*e), axis=-1)\n\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    0.01, decay_steps=1000, decay_rate=0.96, staircase=True\n  )\noptimizer = tf.keras.optimizers.Adagrad(learning_rate=lr_schedule)\n\nquantile = 0.5\nmodel.compile(loss=lambda y,f: tilted_loss(quantile,y,f), optimizer=optimizer, metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"# cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath='cp.ckpt',\n#                                                  save_weights_only=False,\n#                                                  verbose=1)\n\nmodel.fit(trainingData.batch(512), epochs=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get test set\ndata_path = Path('/kaggle/input/geoworld/user_info.test')\n\n# Get datasets from test file\ntext_ds_test, location_ds_test, testData = loadData(data_path)\n\nmodel.evaluate(testData.batch(512))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}